{"cells":[{"cell_type":"markdown","source":"### 贝叶斯新闻分类任务\n\n- 新闻数据集处理\n\n爬取的新闻数据，需要我们对文本数据进行很多预处理才能使用\n\n- 文本分词\n\n通常我们处理的都是词而不是一篇文章\n\n- 去停用词\n\n停用词会对结果产生不好的影响，所以一定得把他们去剔除掉\n\n- 构建文本特征\n\n如何构建合适特征是自然语言处理中最重要的一步，这俩我们选择两种方案来进行对比\n\n- 贝叶斯分类\n\n基于贝叶斯算法来完成最终的分类任务","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport jieba\n#pip install jieba","metadata":{},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### 数据源：http://www.sogou.com/labs/resource/ca.php \n","metadata":{}},{"cell_type":"code","source":"df_news = pd.read_table('./datalab/62820/data.txt',names=['category','theme','URL','content'],encoding='utf-8')\ndf_news = df_news.dropna()\ndf_news.tail()","metadata":{},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"     category                     theme  \\\n4995       时尚              常吃六类食物快速补充水分   \n4996       时尚       情感：你是我的那盘菜　吃不起我走【２】   \n4997       时尚         揭秘不老女神刘晓庆的四任丈夫（图）   \n4998       时尚             样板潮爸　时尚圈里的父亲们   \n4999       时尚  全球最美女人长啥样？中国最美女人酷似章子怡（图）   \n\n                                                    URL  \\\n4995         http://lady.people.com.cn/GB/18248366.html   \n4996  http://lady.people.com.cn/n/2012/0712/c1014-18...   \n4997  http://lady.people.com.cn/n/2012/0730/c1014-18...   \n4998         http://lady.people.com.cn/GB/18215232.html   \n4999  http://lady.people.com.cn/BIG5/n/2012/0727/c10...   \n\n                                                content  \n4995  随着天气逐渐炎热，补水变得日益重要。据美国《跑步世界》杂志报道，喝水并不是为身体补充水分的唯...  \n4996  我其实不想说这些话刺激他，他也是不得已。可是，我又该怎样说，怎样做？我只能走，离开这个伤心地...  \n4997  ５８岁刘晓庆最新嫩照Ｏ衷诘牧跸庆绝对看不出她已经５８岁了，她绝对可以秒杀刘亦菲、范冰冰这类美...  \n4998  导语：做了爸爸就是一种幸福，无论是领养还是亲生，更何况出现在影视剧中。时尚圈永远是需要领军人...  \n4999  全球最美女人合成图：：国整形外科教授李承哲，在国际学术杂志美容整形外科学会学报发表了考虑种族...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>category</th>\n      <th>theme</th>\n      <th>URL</th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4995</th>\n      <td>时尚</td>\n      <td>常吃六类食物快速补充水分</td>\n      <td>http://lady.people.com.cn/GB/18248366.html</td>\n      <td>随着天气逐渐炎热，补水变得日益重要。据美国《跑步世界》杂志报道，喝水并不是为身体补充水分的唯...</td>\n    </tr>\n    <tr>\n      <th>4996</th>\n      <td>时尚</td>\n      <td>情感：你是我的那盘菜　吃不起我走【２】</td>\n      <td>http://lady.people.com.cn/n/2012/0712/c1014-18...</td>\n      <td>我其实不想说这些话刺激他，他也是不得已。可是，我又该怎样说，怎样做？我只能走，离开这个伤心地...</td>\n    </tr>\n    <tr>\n      <th>4997</th>\n      <td>时尚</td>\n      <td>揭秘不老女神刘晓庆的四任丈夫（图）</td>\n      <td>http://lady.people.com.cn/n/2012/0730/c1014-18...</td>\n      <td>５８岁刘晓庆最新嫩照Ｏ衷诘牧跸庆绝对看不出她已经５８岁了，她绝对可以秒杀刘亦菲、范冰冰这类美...</td>\n    </tr>\n    <tr>\n      <th>4998</th>\n      <td>时尚</td>\n      <td>样板潮爸　时尚圈里的父亲们</td>\n      <td>http://lady.people.com.cn/GB/18215232.html</td>\n      <td>导语：做了爸爸就是一种幸福，无论是领养还是亲生，更何况出现在影视剧中。时尚圈永远是需要领军人...</td>\n    </tr>\n    <tr>\n      <th>4999</th>\n      <td>时尚</td>\n      <td>全球最美女人长啥样？中国最美女人酷似章子怡（图）</td>\n      <td>http://lady.people.com.cn/BIG5/n/2012/0727/c10...</td>\n      <td>全球最美女人合成图：：国整形外科教授李承哲，在国际学术杂志美容整形外科学会学报发表了考虑种族...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"\n- Category:当前新闻所属的类别，一会我们要进行分别任务，这就是标签了。\n- Theme:新闻的主题，这个咱们先暂时不用，大家在练习的时候也可以把它当作特征。\n- URL：爬取的界面的链接，方便检验爬取数据是不是完整的，这个咱们暂时也不需要。\n- Content:新闻的内容，这些就是一篇文章了，里面的内容还是很丰富的。","metadata":{}},{"cell_type":"code","source":"df_news.shape","metadata":{},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(5000, 4)"},"metadata":{}}]},{"cell_type":"markdown","source":"###  中文分词\n","metadata":{}},{"cell_type":"code","source":"content = df_news.content.values.tolist() #将每一篇文章转换成一个list\nprint (content[1000]) #随便选择其中一个看看","metadata":{},"execution_count":4,"outputs":[{"name":"stdout","text":"阿里巴巴集团昨日宣布，将在集团管理层面设立首席数据官岗位（Ｃｈｉｅｆ　Ｄａｔａ　Ｏｆｆｉｃｅｒ），阿里巴巴Ｂ２Ｂ公司ＣＥＯ陆兆禧将会出任上述职务，向集团ＣＥＯ马云直接汇报。＞菹ぃ和６月初的首席风险官职务任命相同，首席数据官亦为阿里巴巴集团在完成与雅虎股权谈判，推进“ｏｎｅ　ｃｏｍｐａｎｙ”目标后，在集团决策层面新增的管理岗位。０⒗锛团昨日表示，“变成一家真正意义上的数据公司”已是战略共识。记者刘夏\n","output_type":"stream"}]},{"cell_type":"markdown","source":"使用结巴分词","metadata":{}},{"cell_type":"code","source":"content_S = []\nfor line in content:\n    current_segment = jieba.lcut(line) #对每一篇文章进行分词\n    if len(current_segment) > 1 and current_segment != '\\r\\n': #换行符\n        content_S.append(current_segment) #保存分词的结果","metadata":{},"execution_count":5,"outputs":[{"name":"stderr","text":"Building prefix dict from the default dictionary ...\nDumping model to file cache /tmp/jieba.cache\nLoading model cost 1.335 seconds.\nPrefix dict has been built succesfully.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"看看其中一条的分词结果","metadata":{}},{"cell_type":"code","source":"content_S[1000]","metadata":{},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"['阿里巴巴',\n '集团',\n '昨日',\n '宣布',\n '，',\n '将',\n '在',\n '集团',\n '管理',\n '层面',\n '设立',\n '首席',\n '数据',\n '官',\n '岗位',\n '（',\n 'Ｃ',\n 'ｈ',\n 'ｉ',\n 'ｅ',\n 'ｆ',\n '\\u3000',\n 'Ｄ',\n 'ａ',\n 'ｔ',\n 'ａ',\n '\\u3000',\n 'Ｏ',\n 'ｆ',\n 'ｆ',\n 'ｉ',\n 'ｃ',\n 'ｅ',\n 'ｒ',\n '）',\n '，',\n '阿里巴巴',\n 'Ｂ',\n '２',\n 'Ｂ',\n '公司',\n 'Ｃ',\n 'Ｅ',\n 'Ｏ',\n '陆兆禧',\n '将',\n '会',\n '出任',\n '上述',\n '职务',\n '，',\n '向',\n '集团',\n 'Ｃ',\n 'Ｅ',\n 'Ｏ',\n '马云',\n '直接',\n '汇报',\n '。',\n '＞',\n '菹',\n 'ぃ',\n '和',\n '６',\n '月初',\n '的',\n '首席',\n '风险',\n '官',\n '职务',\n '任命',\n '相同',\n '，',\n '首席',\n '数据',\n '官亦为',\n '阿里巴巴',\n '集团',\n '在',\n '完成',\n '与',\n '雅虎',\n '股权',\n '谈判',\n '，',\n '推进',\n '“',\n 'ｏ',\n 'ｎ',\n 'ｅ',\n '\\u3000',\n 'ｃ',\n 'ｏ',\n 'ｍ',\n 'ｐ',\n 'ａ',\n 'ｎ',\n 'ｙ',\n '”',\n '目标',\n '后',\n '，',\n '在',\n '集团',\n '决策',\n '层面',\n '新增',\n '的',\n '管理',\n '岗位',\n '。',\n '０',\n '⒗',\n '锛',\n '团',\n '昨日',\n '表示',\n '，',\n '“',\n '变成',\n '一家',\n '真正',\n '意义',\n '上',\n '的',\n '数据',\n '公司',\n '”',\n '已',\n '是',\n '战略',\n '共识',\n '。',\n '记者',\n '刘夏']"},"metadata":{}}]},{"cell_type":"code","source":"df_content=pd.DataFrame({'content_S':content_S}) #专门展示分词后的结果\ndf_content.head()","metadata":{},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                           content_S\n0  [经销商, 　, 电话, 　, 试驾, ／, 订车, Ｕ, 憬, 杭州, 滨江区, 江陵, ...\n1  [呼叫, 热线, 　, ４, ０, ０, ８, －, １, ０, ０, －, ３, ０, ０...\n2  [Ｍ, Ｉ, Ｎ, Ｉ, 品牌, 在, 二月, 曾经, 公布, 了, 最新, 的, Ｍ, Ｉ...\n3  [清仓, 大, 甩卖, ！, 一汽, 夏利, Ｎ, ５, 、, 威志, Ｖ, ２, 低至, ...\n4  [在, 今年, ３, 月, 的, 日内瓦, 车展, 上, ，, 我们, 见到, 了, 高尔夫...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content_S</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[经销商, 　, 电话, 　, 试驾, ／, 订车, Ｕ, 憬, 杭州, 滨江区, 江陵, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[呼叫, 热线, 　, ４, ０, ０, ８, －, １, ０, ０, －, ３, ０, ０...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[Ｍ, Ｉ, Ｎ, Ｉ, 品牌, 在, 二月, 曾经, 公布, 了, 最新, 的, Ｍ, Ｉ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[清仓, 大, 甩卖, ！, 一汽, 夏利, Ｎ, ５, 、, 威志, Ｖ, ２, 低至, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[在, 今年, ３, 月, 的, 日内瓦, 车展, 上, ，, 我们, 见到, 了, 高尔夫...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"停用词表","metadata":{}},{"cell_type":"code","source":"stopwords=pd.read_csv(\"./datalab/62820/stopwords.txt\",index_col=False,sep=\"\\t\",quoting=3,names=['stopword'], encoding='utf-8')\nstopwords.head(20)","metadata":{},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"               stopword\n0                     !\n1                     \"\n2                     #\n3                     $\n4                     %\n5                     &\n6                     '\n7                     (\n8                     )\n9                     *\n10                    +\n11                    ,\n12                    -\n13                   --\n14                    .\n15                   ..\n16                  ...\n17               ......\n18  ...................\n19                   ./","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stopword</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>!</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\"</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>#</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>$</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>%</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>&amp;</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>'</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>(</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>)</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>*</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>+</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>,</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>--</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>.</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>..</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>......</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>...................</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>./</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"筛选过滤掉那些停用词","metadata":{}},{"cell_type":"code","source":"def drop_stopwords(contents,stopwords):\n    contents_clean = []\n    all_words = []\n    for line in contents:\n        line_clean = []\n        for word in line:\n            if word in stopwords:\n                continue\n            line_clean.append(word)\n            all_words.append(str(word))\n        contents_clean.append(line_clean)\n    return contents_clean,all_words\n    \ncontents = df_content.content_S.values.tolist()    \nstopwords = stopwords.stopword.values.tolist()\ncontents_clean,all_words = drop_stopwords(contents,stopwords)\n\n#df_content.content_S.isin(stopwords.stopword)\n#df_content=df_content[~df_content.content_S.isin(stopwords.stopword)]\n#df_content.head()","metadata":{},"execution_count":13,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-cfd83c102bd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontents_clean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_content\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent_S\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mcontents_clean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrop_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3613\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3614\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3616\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'content_S'"],"ename":"AttributeError","evalue":"'DataFrame' object has no attribute 'content_S'","output_type":"error"}]},{"cell_type":"code","source":"df_content=pd.DataFrame({'contents_clean':contents_clean})\ndf_content.head()","metadata":{},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                                      contents_clean\n0  [经销商, 电话, 试驾, 订车, Ｕ, 憬, 杭州, 滨江区, 江陵, 路, 号, 转, ...\n1  [呼叫, 热线, 服务, 邮箱, ｋ, ｆ, ｐ, ｅ, ｏ, ｐ, ｌ, ｅ, ｄ, ａ,...\n2  [Ｍ, Ｉ, Ｎ, Ｉ, 品牌, 二月, 公布, 最新, Ｍ, Ｉ, Ｎ, Ｉ, 新, 概念...\n3  [清仓, 甩卖, 一汽, 夏利, Ｎ, 威志, Ｖ, 低至, 万, 启新, 中国, 一汽, ...\n4  [日内瓦, 车展, 见到, 高尔夫, 家族, 新, 成员, 高尔夫, 敞篷版, 款, 全新,...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>contents_clean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[经销商, 电话, 试驾, 订车, Ｕ, 憬, 杭州, 滨江区, 江陵, 路, 号, 转, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[呼叫, 热线, 服务, 邮箱, ｋ, ｆ, ｐ, ｅ, ｏ, ｐ, ｌ, ｅ, ｄ, ａ,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[Ｍ, Ｉ, Ｎ, Ｉ, 品牌, 二月, 公布, 最新, Ｍ, Ｉ, Ｎ, Ｉ, 新, 概念...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[清仓, 甩卖, 一汽, 夏利, Ｎ, 威志, Ｖ, 低至, 万, 启新, 中国, 一汽, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[日内瓦, 车展, 见到, 高尔夫, 家族, 新, 成员, 高尔夫, 敞篷版, 款, 全新,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"就看这里最后一条数据吧，没过滤之前:\n[在, 今年, ３, 月, 的, 日内瓦, 车展, 上, ，, 我们, 见到, 了, 高尔夫...\n过滤之后:\n[日内瓦, 车展, 见到, 高尔夫, 家族, 新, 成员, 高尔夫, 敞篷版, 款, 全新,...\n\n虽然这份停用词表没有做的十分完善，但是已经可以基本完成清洗的任务了，大家可以酌情再来完善这份词表。\n ","metadata":{}},{"cell_type":"markdown","source":"###  TF-IDF ：提取关键词###","metadata":{}},{"cell_type":"code","source":"import jieba.analyse #工具包\nindex = 2400 #随便找一篇文章就行\ncontent_S_str = \"\".join(content_S[index]) #把分词的结果组合在一起，形成一个句子\nprint (content_S_str) #打印这个句子\nprint (\"  \".join(jieba.analyse.extract_tags(content_S_str, topK=5, withWeight=False)))#选出来5个核心词","metadata":{"scrolled":true},"execution_count":15,"outputs":[{"name":"stdout","text":"法国ＶＳ西班牙、里贝里ＶＳ哈维，北京时间６月２４日凌晨一场的大战举世瞩目，而这场胜利不仅仅关乎两支顶级强队的命运，同时也是他们背后的球衣赞助商耐克和阿迪达斯之间的一次角逐。Ｔ谌胙”窘炫分薇的１６支球队之中，阿迪达斯和耐克的势力范围也是几乎旗鼓相当：其中有５家球衣由耐克提供，而阿迪达斯则赞助了６家，此外茵宝有３家，而剩下的两家则由彪马赞助。而当比赛进行到现在，率先挺进四强的两支球队分别被耐克支持的葡萄牙和阿迪达斯支持的德国占据，而由于最后一场１／４决赛是茵宝（英格兰）和彪马（意大利）的对决，这也意味着明天凌晨西班牙同法国这场阿迪达斯和耐克在１／４决赛的唯一一次直接交手将直接决定两家体育巨头在此次欧洲杯上的胜负。８据评估，在２０１２年足球商品的销售额能总共超过４０亿欧元，而单单是不足一个月的欧洲杯就有高达５亿的销售额，也就是说在欧洲杯期间将有７００万件球衣被抢购一空。根据市场评估，两大巨头阿迪达斯和耐克的市场占有率也是并驾齐驱，其中前者占据３８％，而后者占据３６％。体育权利顾问奥利弗－米歇尔在接受《队报》采访时说：“欧洲杯是耐克通过法国翻身的一个绝佳机会！”Ｃ仔尔接着谈到两大赞助商的经营策略：“竞技体育的成功会燃起球衣购买的热情，不过即便是水平相当，不同国家之间的欧洲杯效应却存在不同。在德国就很出色，大约１／４的德国人通过电视观看了比赛，而在西班牙效果则差很多，由于民族主义高涨的加泰罗尼亚地区只关注巴萨和巴萨的球衣，他们对西班牙国家队根本没什么兴趣。”因此尽管西班牙接连拿下欧洲杯和世界杯，但是阿迪达斯只为西班牙足协支付每年２６００万的赞助费＃相比之下尽管最近两届大赛表现糟糕法国足协将从耐克手中每年可以得到４０００万欧元。米歇尔解释道：“法国创纪录的４０００万欧元赞助费得益于阿迪达斯和耐克竞逐未来１５年欧洲市场的竞争。耐克需要笼络一个大国来打赢这场欧洲大陆的战争，而尽管德国拿到的赞助费并不太高，但是他们却显然牢牢掌握在民族品牌阿迪达斯手中。从长期投资来看，耐克给法国的赞助并不算过高。”\n耐克  阿迪达斯  欧洲杯  球衣  西班牙\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train=pd.DataFrame({'contents_clean':contents_clean,'label':df_news['category']})\ndf_train.tail()","metadata":{},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"                                         contents_clean label\n4995  [天气, 炎热, 补水, 变得, 美国, 跑步, 世界, 杂志, 报道, 喝水, 身体, 补...    时尚\n4996  [不想, 说, 话, 刺激, 说, 做, 只能, 走, 离开, 伤心地, 想起, 一句, 话...    时尚\n4997  [岁, 刘晓庆, 最新, 嫩照, Ｏ, 衷, 诘, 牧跸, 庆, 看不出, 岁, 秒杀, 刘...    时尚\n4998  [导语, 做, 爸爸, 一种, 幸福, 无论是, 领养, 亲生, 更何况, 影视剧, 中, ...    时尚\n4999  [全球, 最美, 女人, 合成图, 国, 整形外科, 教授, 李承哲, 国际, 学术, 杂志...    时尚","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>contents_clean</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4995</th>\n      <td>[天气, 炎热, 补水, 变得, 美国, 跑步, 世界, 杂志, 报道, 喝水, 身体, 补...</td>\n      <td>时尚</td>\n    </tr>\n    <tr>\n      <th>4996</th>\n      <td>[不想, 说, 话, 刺激, 说, 做, 只能, 走, 离开, 伤心地, 想起, 一句, 话...</td>\n      <td>时尚</td>\n    </tr>\n    <tr>\n      <th>4997</th>\n      <td>[岁, 刘晓庆, 最新, 嫩照, Ｏ, 衷, 诘, 牧跸, 庆, 看不出, 岁, 秒杀, 刘...</td>\n      <td>时尚</td>\n    </tr>\n    <tr>\n      <th>4998</th>\n      <td>[导语, 做, 爸爸, 一种, 幸福, 无论是, 领养, 亲生, 更何况, 影视剧, 中, ...</td>\n      <td>时尚</td>\n    </tr>\n    <tr>\n      <th>4999</th>\n      <td>[全球, 最美, 女人, 合成图, 国, 整形外科, 教授, 李承哲, 国际, 学术, 杂志...</td>\n      <td>时尚</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"数据集标签制作","metadata":{}},{"cell_type":"code","source":"df_train.label.unique()","metadata":{},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"array(['汽车', '财经', '科技', '健康', '体育', '教育', '文化', '军事', '娱乐', '时尚'], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"label_mapping = {\"汽车\": 1, \"财经\": 2, \"科技\": 3, \"健康\": 4, \"体育\":5, \"教育\": 6,\"文化\": 7,\"军事\": 8,\"娱乐\": 9,\"时尚\": 0}\ndf_train['label'] = df_train['label'].map(label_mapping) #构建一个映射方法\ndf_train.head()","metadata":{},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"                                      contents_clean  label\n0  [经销商, 电话, 试驾, 订车, Ｕ, 憬, 杭州, 滨江区, 江陵, 路, 号, 转, ...      1\n1  [呼叫, 热线, 服务, 邮箱, ｋ, ｆ, ｐ, ｅ, ｏ, ｐ, ｌ, ｅ, ｄ, ａ,...      1\n2  [Ｍ, Ｉ, Ｎ, Ｉ, 品牌, 二月, 公布, 最新, Ｍ, Ｉ, Ｎ, Ｉ, 新, 概念...      1\n3  [清仓, 甩卖, 一汽, 夏利, Ｎ, 威志, Ｖ, 低至, 万, 启新, 中国, 一汽, ...      1\n4  [日内瓦, 车展, 见到, 高尔夫, 家族, 新, 成员, 高尔夫, 敞篷版, 款, 全新,...      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>contents_clean</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[经销商, 电话, 试驾, 订车, Ｕ, 憬, 杭州, 滨江区, 江陵, 路, 号, 转, ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[呼叫, 热线, 服务, 邮箱, ｋ, ｆ, ｐ, ｅ, ｏ, ｐ, ｌ, ｅ, ｄ, ａ,...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[Ｍ, Ｉ, Ｎ, Ｉ, 品牌, 二月, 公布, 最新, Ｍ, Ｉ, Ｎ, Ｉ, 新, 概念...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[清仓, 甩卖, 一汽, 夏利, Ｎ, 威志, Ｖ, 低至, 万, 启新, 中国, 一汽, ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[日内瓦, 车展, 见到, 高尔夫, 家族, 新, 成员, 高尔夫, 敞篷版, 款, 全新,...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(df_train['contents_clean'].values, df_train['label'].values, random_state=1)","metadata":{},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#x_train = x_train.flatten()\nx_train[0][1]","metadata":{},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"'上海'"},"metadata":{}}]},{"cell_type":"code","source":"words = []\nfor line_index in range(len(x_train)):\n    try:\n        #x_train[line_index][word_index] = str(x_train[line_index][word_index])\n        words.append(' '.join(x_train[line_index]))\n    except:\n        print (line_index,word_index)\nwords[0]        ","metadata":{},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"'中新网 上海 日电 于俊 父亲节 网络 吃 一顿 电影 快餐 微 电影 爸 对不起 我爱你 定于 本月 父亲节 当天 各大 视频 网站 首映 葜 谱 鞣 剑 保慈 障蚣 钦 呓 樯 埽 ⒌ 缬 埃 ǎ 停 椋 悖 颍 铩 妫 椋 恚 称 微型 电影 新 媒体 平台 播放 状态 短时 休闲 状态 观看 完整 策划 系统 制作 体系 支持 显示 较完整 故事情节 电影 微 超短 放映 微 周期 制作 天 数周 微 规模 投资 人民币 几千 数万元 每部 内容 融合 幽默 搞怪 时尚 潮流 人文 言情 公益 教育 商业 定制 主题 单独 成篇 系列 成剧 唇 开播 微 电影 爸 对不起 我爱你 讲述 一对 父子 观念 缺少 沟通 导致 关系 父亲 传统 固执 钟情 传统 生活 方式 儿子 新派 音乐 达 习惯 晚出 早 生活 性格 张扬 叛逆 两种 截然不同 生活 方式 理念 差异 一场 父子 间 拉开序幕 子 失手 打破 父亲 心爱 物品 父亲 赶出 家门 剧情 演绎 父亲节 妹妹 哥哥 化解 父亲 这场 矛盾 映逋坏 嚼 斫 狻 ⒍ 粤 ⒌ 桨容 争执 退让 传统 尴尬 父子 尴尬 情 男人 表达 心中 那份 感恩 一杯 滤挂 咖啡 父亲节 变得 温馨 镁 缬 缮 虾 Ｎ 逄 煳 幕 传播 迪欧 咖啡 联合 出品 出品人 希望 观摩 扪心自问 父亲节 父亲 记得 父亲 生日 哪一天 父亲 爱喝 跨出 家门 那一刻 感觉 一颗 颤动 心 操劳 天下 儿女 父亲节 大声 喊出 父亲 家人 爱 完'"},"metadata":{}}]},{"cell_type":"code","source":"print (len(words))","metadata":{},"execution_count":22,"outputs":[{"name":"stdout","text":"3750\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 制作词袋模型特征","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ntexts=[\"dog cat fish\",\"dog cat cat\",\"fish bird\", 'bird'] #为了简单期间，这里4句话我们就当做4篇文章了\ncv = CountVectorizer() #词频统计\ncv_fit=cv.fit_transform(texts) #转换数据\n\nprint(cv.get_feature_names())\nprint(cv_fit.toarray())\n\n\nprint(cv_fit.toarray().sum(axis=0))","metadata":{},"execution_count":23,"outputs":[{"name":"stdout","text":"['bird', 'cat', 'dog', 'fish']\n[[0 1 1 1]\n [0 2 1 0]\n [1 0 0 1]\n [1 0 0 0]]\n[2 3 2 2]\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ntexts=[\"dog cat fish\",\"dog cat cat\",\"fish bird\", 'bird']\ncv = CountVectorizer(ngram_range=(1,4)) #设置ngram参数，让结果不光包含一个词，还有2个，3个的组合\ncv_fit=cv.fit_transform(texts)\n\nprint(cv.get_feature_names())\nprint(cv_fit.toarray())\n\n\nprint(cv_fit.toarray().sum(axis=0))","metadata":{},"execution_count":24,"outputs":[{"name":"stdout","text":"['bird', 'cat', 'cat cat', 'cat fish', 'dog', 'dog cat', 'dog cat cat', 'dog cat fish', 'fish', 'fish bird']\n[[0 1 0 1 1 1 0 1 1 0]\n [0 2 1 0 1 1 1 0 0 0]\n [1 0 0 0 0 0 0 0 1 1]\n [1 0 0 0 0 0 0 0 0 0]]\n[2 3 1 1 2 2 1 1 2 1]\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nvec = CountVectorizer(analyzer='word',lowercase = False)\nfeature = vec.fit_transform(words)\nfeature.shape","metadata":{},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"(3750, 85093)"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nvec = CountVectorizer(analyzer='word', max_features=4000,  lowercase = False)\nfeature = vec.fit_transform(words)\n","metadata":{},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"feature.shape","metadata":{},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"(3750, 4000)"},"metadata":{}}]},{"cell_type":"markdown","source":"在构建过程中，我们还额外加入了一个限制条件max_features=4000，表示我们的特征最大长度为4000，这就会自动过滤掉一些词频较小的词语了。如果不进行限制的话，最终得到的向量长度为85093，大家也可以去掉这个参数来自己观察下，这会使得特征长度过大，而且里面很多都是词频很低的词语，也会导致特征过于稀疏，这些对我们建模来说都是不利的，所以还是非常有必要加上这样一个限制参数。","metadata":{}},{"cell_type":"markdown","source":"### 使用词袋模型的特征来建模，观察结果","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB #贝叶斯模型\nclassifier = MultinomialNB() \nclassifier.fit(feature, y_train)","metadata":{},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"},"metadata":{}}]},{"cell_type":"code","source":"test_words = []\nfor line_index in range(len(x_test)):\n    try:\n        #\n        test_words.append(' '.join(x_test[line_index]))\n    except:\n         print (line_index,word_index)\ntest_words[0]","metadata":{},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"'国家 公务员 考试 申论 应用文 类 试题 实质 一道 集 概括 分析 提出 解决问题 一体 综合性 试题 说 一道 客观 凝练 申发 论述 文章 题目 分析 历年 国考 申论 真题 公文 类 试题 类型 多样 包括 公文 类 事务性 文书 类 题材 从题 干 作答 材料 内容 整合 分析 无需 太 创造性 发挥 纵观 历年 申论 真题 作答 应用文 类 试题 文种 格式 作出 特别 重在 内容 考查 行文 格式 考生 平常心 面对 应用文 类 试题 准确 把握 作答 领会 内在 含义 把握 题材 主旨 材料 结构 轻松 应对 应用文 类 试题 Ｒ 弧 ⒆ 钒 盐 展文 写作 原则 Ｔ 材料 中来 应用文 类 试题 材料 总体 把握 客观 考生 材料 中来 材料 中 把握 材料 准确 理解 题材 主旨 Ｔ 政府 角度 作答 应用文 类 试题 更应 注重 政府 角度 观点 政府 角度 出发 原则 表述 观点 提出 解决 之策 考生 作答 站 政府 人员 角度 看待 提出 解决问题 Ｔ 文体 结构 形式 考查 重点 文体 结构 大部分 评分 关键点 解答 方法 薄 ⒆ ス 丶 词 明 方向 作答 题目 题干 作答 作答 方向 作答 角度 关键 向导 考生 仔细阅读 题干 作答 抓住 关键词 作答 方向 相关 要点 整理 作答 思路 年国考 地市级 真 题为 例 潦惺姓 府 宣传 推进 近海 水域 污染 整治 工作 请 给定 资料 市政府 工作人员 身份 草拟 一份 宣传 纲要 Ｒ 求 保对 宣传 内容 要点 提纲挈领 陈述 玻 体现 政府 精神 全市 各界 关心 支持 污染 整治 工作 通俗易懂 超过 字 肮 丶 词 近海 水域 污染 整治 工作 市政府 工作人员 身份 宣传 纲要 提纲挈领 陈述 体现 政府 精神 全市 各界 关心 支持 污染 整治 工作 通俗易懂 提示 归结 作答 要点 包括 污染 情况 原因 解决 对策 作答 思路 情况 原因 对策 意义 逻辑 顺序 安排 文章 结构 病 ⒋ 缶殖 龇 ⅲ 明 结构 解答 应用文 类 试题 考生 材料 整体 出发 大局 出发 高屋建瓴 把握 材料 主题 思想 事件 起因 解决 对策 阅读文章 构建 文章 结构 直至 快速 解答 场 ⒗ 硭 乘悸 罚明 逻辑 应用文 类 试题 严密 逻辑思维 情况 原因 对策 意义 考生 作答 先 弄清楚 解答 思路 统筹安排 脉络 清晰 逻辑 表达 内容 表述 础 把握 明 详略 考生 仔细阅读 分析 揣摩 应用文 类 试题 内容 答题 时要 详略 得当 主次 分明 安排 内容 增加 文章 层次感 阅卷 老师 阅卷 时能 明白 清晰 一目了然 玻埃 保蹦旯 考 考试 申论 试卷 分为 省级 地市级 两套 试卷 能力 大有 省级 申论 试题 考生 宏观 角度看 注重 深度 广度 考生 深谋远虑 地市级 试题 考生 微观 视角 观察 侧重 考查 解决 能力 考生 贯彻执行 作答 区别对待'"},"metadata":{}}]},{"cell_type":"code","source":"classifier.score(vec.transform(test_words), y_test)","metadata":{},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"0.80400000000000005"},"metadata":{}}]},{"cell_type":"markdown","source":"在贝叶斯模型中，我们选择了MultinomialNB，这里它额外做了一些平滑处理主要目的就在我们求解先验概率和条件概率的时候避免其值为0。","metadata":{}},{"cell_type":"markdown","source":"### 制作TF-IDF特征","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nX_test = ['卡尔 敌法师 蓝胖子 小小','卡尔 敌法师 蓝胖子 痛苦女王']\n\ntfidf=TfidfVectorizer()\nweight=tfidf.fit_transform(X_test).toarray()\nword=tfidf.get_feature_names()\nprint (weight)\nfor i in range(len(weight)):  \n    print (u\"第\", i, u\"篇文章的tf-idf权重特征\")\n    for j in range(len(word)):\n        print (word[j], weight[i][j])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 使用TF-IDF特征建模来观察结果","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(analyzer='word', max_features=4000,  lowercase = False)\nvectorizer.fit(words)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB()\nclassifier.fit(vectorizer.transform(words), y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier.score(vectorizer.transform(test_words), y_test)","metadata":{},"execution_count":null,"outputs":[]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}